{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasterRCNN_helmet.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPVuSyXGpmC8Go7B8fbG2D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/biggymart/Group_proj/blob/master/fasterRCNN_helmet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICUc4MEjI717"
      },
      "source": [
        "# **동빈나 YouTube**\n",
        "**RPN** : ConvNet -> GPU, faster than CPU, only discerns whether an object exists\n",
        "\n",
        "2015년도 기준으로 **5FPS** 정도면 아주 빠른 속도였음\n",
        "\n",
        "keywords: Objectness scores, A pyramid of anchors, Alternating training, feature sharing, Translation invariant, NMS (IOU 0.7, 2000 proposals)\n",
        "\n",
        "Architecture: Two modules: (1) RPN (2) fast RCNN\n",
        "\n",
        "k boxes -> cls 2k, reg 4k\n",
        "\n",
        "IOU 0.7 이상: positive; 0.3 이하: negative;\n",
        "\n",
        "PASCAL VOC 2007 (5k train; 5k test) -> 59.9%; Ablation Experiment\n",
        "\n",
        "07 + 12 -> 70%, two-stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NLU8XFqZzmk"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itpg1xMIaRP0",
        "outputId": "b8470569-1714-4345-fa30-29edf9b9f7a2"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torh.device('cuda')\n",
        "  print(device, torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "  print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk78XhKtaqGj"
      },
      "source": [
        "Read a batch of trainingdimages along with their bounding boxes and labels.\n",
        "In this example, I only read 1 image. (i.e batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "dh0DoEBXbEqs",
        "outputId": "b349005b-769e-4228-fdd0-dbf5e3280789"
      },
      "source": [
        "# input image could be of any size\n",
        "img0 = cv2.imread('helmet_9.jpg')\n",
        "img0 = cv2.cvtColor(img0, COLOR_BGR2RGB)\n",
        "print(img0.shape)\n",
        "plt.imshow(img0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ebd41f3de109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# input image could be of any size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helmet_9.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'COLOR_BGR2RGB' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L40la640cHmS"
      },
      "source": [
        "# Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels\n",
        "bbox0 = np.array([160, 147, 260, 234], [139, 312, 200, 348])\n",
        "labels = np.array([1, 1]) # 0: background, 1: helmet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "t8G0xLjEce2L",
        "outputId": "390cf421-54e6-4469-b9eb-d88e873203c2"
      },
      "source": [
        "# display bounding box and labels\n",
        "# cv2.putText(TextYouWantToPrint)\n",
        "img0_clone = np.copy(img0)\n",
        "for i in range(len(bbox0)):\n",
        "  cv2.rectangle(img0_clone, (bbox0[i][1], bbox0[i][0]), (bbox0[i][3], bbox0[i][2]), color=(0, 255, 0), thickness=3)\n",
        "  cv2.putText(img0_clone, str(int(labels[i])), (bbox0[i][3], bbox0[i][2]), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255), thickness=3)\n",
        "plt.imshow(img0_clone)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7211bb09212c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# cv2.putText(TextYouWantToPrint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_clone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_clone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthickness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bbox' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hsgo9VIfd4W9"
      },
      "source": [
        "Resize the input images to (h=800, w=800)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjuKH_FieQxY"
      },
      "source": [
        "img = cv2.resize(img0, dsize=(800, 800), interpolation=cv2.INTER_CUBIC)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_2bUg_cfNWG"
      },
      "source": [
        "# change the bounding box coordinates\n",
        "Wratio = 800/img0.shape[1]\n",
        "Hratio = 800/img0.shape[0]\n",
        "ratioLst = [Hratio, Wratio, Hratio, Wratio]\n",
        "bbox = []\n",
        "for box in bbox0:\n",
        "  box = [int(a * b) for a, b in zip(box, ratioLst)]\n",
        "  bbox.append(box)\n",
        "bbox = np.array(bbox)\n",
        "print(bbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFzkJBj7gN37"
      },
      "source": [
        "# display bounding box and labels\n",
        "img_clone = np.copy(img)\n",
        "bbox_clone = bbox.astype(int)\n",
        "for i in range(len(bbox)):\n",
        "  # Draw rectangle\n",
        "  cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3)\n",
        "  # Write the prediction\n",
        "  cv2.putText(img_clone, str(int(labels[i])), (bbox[i][3], bbox[i][2]), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255), thickness=3)\n",
        "plt.imshow(img_clone)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3j-mq3wvNVK"
      },
      "source": [
        "Use VGG16 to extract features from input images\n",
        "Input images (batch_size, H=800, W=800, d=3), Features: (batch_size, H=50, W=50, d=512)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4iFiOUuvfSB"
      },
      "source": [
        "# List all the layers of VGG16\n",
        "model = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "fc = list(model.features)\n",
        "print(len(fc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ITAC7YQv3JY"
      },
      "source": [
        "# Collect layers with output feature map size (W, H) < 50\n",
        "# test image array [1, 3, 800, 800]\n",
        "dummy_img = torch.zeros((1, 3, 800, 800)).float()\n",
        "print(dummy_img)\n",
        "\n",
        "req_features = []\n",
        "k = dummy_img.clone().to(device)\n",
        "for i in fc:\n",
        "  k = i(k)\n",
        "  if k.size()[2] < 800//16: # 800/16 = 50\n",
        "    break\n",
        "  req_features.append(i)\n",
        "  out_channels = k.size()[1]\n",
        "print(len(req_features)) # 30\n",
        "print(out_channels) # 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVo30UuzxrKJ"
      },
      "source": [
        "# Convert this list into a Sequential module\n",
        "faster_rcnn_fc_extractor = nn.Sequential(*req_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGcA2EFfx2gD"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()]) # Define PyTorch Transform\n",
        "imgTensor = transform(img).to(device)\n",
        "imgTensor = imgTensor.unsqueeze(0)\n",
        "out_map = faster_rcnn_fc_extractor(imgTensor)\n",
        "print(out_map.size())\n",
        "# torch.Size([1, 512, 50, 50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvwkuRptyTJV"
      },
      "source": [
        "# visualize the first 5 channels of the 50*50*512 feature maps\n",
        "imgArray = out_map.cpu().numpy().squeeze(0)\n",
        "fig = plt.figure(figsize=(12, 4))\n",
        "figNo = 1\n",
        "for i in range(5):\n",
        "  fig.add_subplot(1, 5, figNo)\n",
        "  plt.imshow(imgArray[i], cmap='gray')\n",
        "  figNo += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOxgNqhcy8AA"
      },
      "source": [
        "Generate 22,500 anchor boxes on each input image\n",
        "50x50=2500 anchors, each anchor generates 9 anchor boxes, Total = 50x50x9=22,500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6GQAOVhzSRr"
      },
      "source": [
        "# x, y intervals to generate anchor box center\n",
        "fc_size = (800//16)\n",
        "ctr_x = np.arange(16, (fc_size+1)*16, 16)\n",
        "ctr_y = np.arange(16, (fc_size+1)*16, 16)\n",
        "print(len(ctr_x), ctr_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnsMX-opzoyx"
      },
      "source": [
        "# coordinates of the 2500 center points to generate anchor boxes\n",
        "index = 0\n",
        "ctr = np.zeros((2500,2))\n",
        "for x in range(len(ctr_x)):\n",
        "  for y in range(len(ctr_y)):\n",
        "    ctr[index, 1] = ctr_x[x] - 8\n",
        "    ctr[index, 0] = ctr_y[x] - 8\n",
        "    index += 1\n",
        "print(ctr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBX5sJvp0IqS"
      },
      "source": [
        "# display the 2500 anchors\n",
        "img_clone = np.copy(img)\n",
        "plt.figure(figsize=(9,6))\n",
        "for i in range(ctr.shape[0]):\n",
        "  cv2.circle(img_clone, (int(ctr[i][0]), int(ctr[i][1])), radius=1, color=(255,0,0), thickness=1)\n",
        "plt.imshow(img_clone)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdQIV0P908U_"
      },
      "source": [
        "# for each of the 2500 anchors, generate 9 anchor boxes\n",
        "# 2500 * 9 = 25,000 anchor boxes\n",
        "ratios = [0.5, 1, 2]\n",
        "scales = [8, 16, 32]\n",
        "sub_samples = 16\n",
        "anchor_boxes = np.zeros(((fc_size * fc_size * 9), 4))\n",
        "index = 0\n",
        "for c in ctr:\n",
        "  ctr_y, ctr_x = c\n",
        "  for i in range(len(ratios)):\n",
        "    for j in range(len(scales)):\n",
        "      h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
        "      w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
        "      anchor_boxes[index, 0] = ctr_y - h /2.\n",
        "      anchor_boxes[index, 1] = ctr_x - w /2.\n",
        "      anchor_boxes[index, 2] = ctr_y - h /2.\n",
        "      anchor_boxes[index, 3] = ctr_x - w /2.\n",
        "      index += 1\n",
        "print(anchor_boxes.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}